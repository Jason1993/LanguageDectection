{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now processing sl...\n",
      "Now processing sk...\n",
      "Now processing pl...\n",
      "Now processing sv...\n",
      "Now processing da...\n",
      "Now processing el...\n",
      "Now processing lv...\n",
      "Now processing it...\n",
      "Now processing cs...\n",
      "Now processing ro...\n",
      "Now processing pt...\n",
      "Now processing hu...\n",
      "Now processing nl...\n",
      "Now processing bg...\n",
      "Now processing de...\n",
      "Now processing fi...\n",
      "Now processing fr...\n",
      "Now processing es...\n",
      "Now processing et...\n",
      "Now processing en...\n",
      "Now processing lt...\n",
      "N-Gram Model building finshied\n",
      "The accuracy is: 0.9515238095238095\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import operator\n",
    "import re\n",
    "import codecs\n",
    "import json\n",
    "\n",
    "def getWords(text):\n",
    "    return re.compile('\\w+').findall(text)\n",
    "\n",
    "def buildModel():\n",
    "    '''\n",
    "    This model uses zipf's law to help decide the category of document;\n",
    "    This model contains the most 50 frequent occurance N-grams in each language.\n",
    "    Return: uniGramModel, biGramModel, triGramModel\n",
    "    '''\n",
    "    langList = []\n",
    "    #Jupyter network file path:./Desktop/NLP/txt/\n",
    "    #Need to modify path when run on your own environment.\n",
    "    path = './Desktop/NLP/txt/'\n",
    "    for name in os.listdir(path):\n",
    "        if len(name) == 2:\n",
    "            langList.append(name)\n",
    "    uniGramodel = {}\n",
    "    biGramodel = {}\n",
    "    triGramodel = {}\n",
    "    for lang in langList:\n",
    "        print(\"Now processing \" + lang+\"...\")\n",
    "        uniGramodel[lang] = {}\n",
    "        biGramodel[lang] = {}\n",
    "        triGramodel[lang] = {}\n",
    "        for filename in os.listdir(path+lang+\"/\"):\n",
    "            with codecs.open(path+lang+\"/\"+filename, \"r\", encoding='utf-8', errors='ignore') as fdata:\n",
    "                for line in fdata:\n",
    "                    #print(type(line))\n",
    "                    #print(\"Program is running\")\n",
    "                    if line[0] == '<':\n",
    "                        continue\n",
    "                    phrases = line.split('.')\n",
    "                    for phra in phrases:\n",
    "                        wordList = getWords(phra)\n",
    "                        for i,word in enumerate(wordList):\n",
    "                            word = word.strip()\n",
    "                            if i == 0:\n",
    "                                biGram = (\"\",word)\n",
    "                                if i+1 < len(wordList):\n",
    "                                    triGram = (\"\",word,wordList[i+1].strip())\n",
    "                                else:\n",
    "                                    triGram = None\n",
    "                            elif i == len(wordList)-1:\n",
    "                                biGram = (word,\"\")\n",
    "                                triGram = (wordList[i-1].strip(),word,\"\")\n",
    "                            else:\n",
    "                                biGram = (word,wordList[i+1])\n",
    "                                if i+2 < len(wordList):\n",
    "                                    triGram = (wordList[i],wordList[i+1],wordList[i+2])\n",
    "                                else:\n",
    "                                    triGram = None\n",
    "                            if word not in uniGramodel[lang]:\n",
    "                                uniGramodel[lang][word] = 1\n",
    "                            else:\n",
    "                                uniGramodel[lang][word] += 1\n",
    "                            if biGram not in biGramodel[lang]:\n",
    "                                biGramodel[lang][biGram] = 1\n",
    "                            else:\n",
    "                                biGramodel[lang][biGram] += 1\n",
    "                            if triGram != None:\n",
    "                                if triGram not in triGramodel[lang]:\n",
    "                                    triGramodel[lang][triGram] = 1\n",
    "                                else:\n",
    "                                    triGramodel[lang][triGram] += 1\n",
    "    uniGramList = {}\n",
    "    biGramList = {}\n",
    "    triGramList = {}\n",
    "    for lang in langList:\n",
    "        uniGramodel[lang] = sorted(uniGramodel[lang].items(), key = operator.itemgetter(1), reverse = True)\n",
    "        biGramodel[lang] = sorted(biGramodel[lang].items(), key = operator.itemgetter(1), reverse = True)\n",
    "        triGramodel[lang] = sorted(triGramodel[lang].items(), key = operator.itemgetter(1), reverse = True)\n",
    "        uniGramList[lang] = []\n",
    "        biGramList[lang] = []\n",
    "        triGramList[lang] = []\n",
    "        for i in range(50):\n",
    "            uniGramList[lang].append(uniGramodel[lang][i][0])\n",
    "            biGramList[lang].append(biGramodel[lang][i][0])\n",
    "            triGramList[lang].append(triGramodel[lang][i][0])\n",
    "    print(\"N-Gram Model building finshied\")\n",
    "\n",
    "    return uniGramList,biGramList,triGramList,langList\n",
    "\n",
    "#uni,bi,tri = buildModel()\n",
    "def getLabel(uniGramList, biGramList, triGramList, document,langList):\n",
    "    '''\n",
    "    This function is to calculate the distance from document to all possible languages.\n",
    "    The document will be labeled as the shortest distance language.\n",
    "    Params:\n",
    "    uniGramList, biGramList, triGramList, document\n",
    "    Return:\n",
    "    document language label\n",
    "    '''\n",
    "    phrases = document.split('.')\n",
    "    docUni = {}\n",
    "    docBi = {}\n",
    "    docTri = {}\n",
    "    for phra in phrases:\n",
    "        wordList = getWords(phra)\n",
    "        for i,word in enumerate(wordList):\n",
    "            word = word.strip()\n",
    "            if i == 0:\n",
    "                biGram = (\"\", word)\n",
    "                if i + 1 < len(wordList):\n",
    "                    triGram = (\"\", word, wordList[i + 1].strip())\n",
    "                else:\n",
    "                    triGram = None\n",
    "            elif i == len(wordList) - 1:\n",
    "                biGram = (word, \"\")\n",
    "                triGram = (wordList[i - 1].strip(), word, \"\")\n",
    "            else:\n",
    "                biGram = (word, wordList[i + 1])\n",
    "                if i + 2 < len(wordList):\n",
    "                    triGram = (wordList[i], wordList[i + 1], wordList[i + 2])\n",
    "                else:\n",
    "                    triGram = None\n",
    "            if word not in docUni:\n",
    "                docUni[word] = 1\n",
    "            else:\n",
    "                docUni[word] += 1\n",
    "            if biGram not in docBi:\n",
    "                docBi[biGram] = 1\n",
    "            else:\n",
    "                docBi[biGram] += 1\n",
    "            if triGram != None:\n",
    "                if triGram not in docTri:\n",
    "                    docTri[triGram] = 1\n",
    "                else:\n",
    "                    docTri[triGram] += 1\n",
    "    docUni = sorted(docUni.items(), key = operator.itemgetter(1), reverse = True)\n",
    "    docBi = sorted(docBi.items(), key=operator.itemgetter(1), reverse=True)\n",
    "    docTri = sorted(docTri.items(), key=operator.itemgetter(1), reverse=True)\n",
    "    disUni = {}\n",
    "    disBi = {}\n",
    "    disTri = {}\n",
    "    score = {}\n",
    "    for lang in langList:\n",
    "        disUni[lang] = 0\n",
    "        disBi[lang] = 0\n",
    "        disTri[lang] = 0\n",
    "        for i in range(len(docUni)):\n",
    "            if docUni[i][0] in uniGramList[lang]:\n",
    "                disUni[lang] += abs(uniGramList[lang].index(docUni[i][0])-i)\n",
    "            else:\n",
    "                disUni[lang] += 50\n",
    "        for i in range(len(docBi)):\n",
    "            if docBi[i][0] in biGramList[lang]:\n",
    "                disBi[lang] += abs(biGramList[lang].index(docBi[i][0])-i)\n",
    "            else:\n",
    "                disBi[lang] += 50\n",
    "        for i in range(len(docTri)):\n",
    "            if docTri[i][0] in triGramList[lang]:\n",
    "                disTri[lang] += abs(triGramList[lang].index(docTri[i][0])-i)\n",
    "            else:\n",
    "                disTri[lang] += 50\n",
    "        score[lang] = disUni[lang]+disBi[lang]+disTri[lang]\n",
    "    score = sorted(score.items(), key = operator.itemgetter(1))\n",
    "    return score[0][0]\n",
    "\n",
    "\n",
    "def main():\n",
    "    uni,bi,tri,lang = buildModel()\n",
    "    correct = 0\n",
    "    count = 0\n",
    "    with codecs.open(\"./PycharmProjects/LanguageCat/europarl.test\", \"r\", encoding='utf-8', errors='ignore') as fdata:\n",
    "        for i,line in enumerate(fdata):\n",
    "            doc = line[3:]\n",
    "            label = getLabel(uni,bi,tri,doc,lang)\n",
    "            if label == line[0:2]:\n",
    "                correct += 1\n",
    "            count += 1\n",
    "    print(\"The accuracy is: \"+str(correct/count))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This program above builds a N-Gram model, which more specifically is uni-Gram, bi-Gram and triGram for each language in the corpus. It stores top 50 most frequent uni-Grams, biGrams and triGrams for each language.\n",
    "\n",
    "The intuition for building a N-Gram model to solve this problem is based on Zipf's law. Zipf's law states that given a large sample of words used, the frequency of any word is inversely proportional to its rank in the frequency table. Thus we build a language model based on N-Gram.\n",
    "\n",
    "When we are given the N-Gram model, the next step we need to do is for each unlabeled document, we count and sort its uni-Grams, bi-Grams, and tri-Grams. Then compare document's uni-Grams, bi-Grams and tri-Grams with the N-Gram model we have built, and the document will be labeled as the language model that has shortest distance to the document.\n",
    "\n",
    "As can be seen from the output of the program, the accuracy of test data set is 95.15%, it is acceptable, considering the model we built is a simple model. However, there are stills some ways to improve the accuracy, for exmaple, we can pick more than 50 most frequent N-Grams to achieve higher accuracy. Also when doing natural language processing of the corpus, we can do more detailed filtering of the text, insteading of just using regex to extract words."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
